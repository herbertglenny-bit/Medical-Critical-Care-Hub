import streamlit as st
import google.generativeai as genai
import PyPDF2

# --- CONFIGURACI√ìN DE LA P√ÅGINA (NOMBRE EN PESTA√ëA NAVEGADOR) ---
st.set_page_config(
    page_title="Medical Critical Care Hub",
    page_icon="üè•",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- PROMPTS MAESTROS (INTENSIVISTA & DISE√ëO) ---
PROMPT_ANALISIS = """
# ROL
Act√∫a como un M√©dico Intensivista Senior y Experto en Educaci√≥n M√©dica Universitaria.
# OBJETIVO
Analizar en profundidad la Gu√≠a de Pr√°ctica Cl√≠nica (GPC) adjunta para una sesi√≥n cl√≠nica.
# INSTRUCCIONES DE AN√ÅLISIS
Genera un informe estructurado que cubra los siguientes puntos clave.
## 1. Ficha T√©cnica Resumida
* T√≠tulo, Sociedad, A√±o, Poblaci√≥n, Metodolog√≠a.
## 2. An√°lisis Delta: ¬øQu√© hay de nuevo?
* Nuevas Recomendaciones Fuertes.
* Conceptos Obsoletos (Lo que debemos dejar de hacer).
* Cambios en Dosis/Umbrales.
## 3. Algoritmo de Manejo Pr√°ctico (Bedside)
* Fase de Resucitaci√≥n/Aguda.
* Fase de Mantenimiento.
* Fase de Destete/Salida.
## 4. Rinc√≥n del Residente (Docencia)
* 3 "Key Learning Points".
* 3 Preguntas de Guardia (tipo test/caso corto con respuesta).
* Evidencia Clave (Ensayos cl√≠nicos mencionados).
## 5. √Åreas de Incertidumbre
"""

PROMPT_INFOGRAFIA = """
# ROL
Act√∫a como un Experto en Comunicaci√≥n Cient√≠fica Visual y M√©dico Intensivista.
# OBJETIVO
Estructurar la informaci√≥n de la Gu√≠a para crear una Infograf√≠a T√©cnica de Alto Impacto (One-Page Visual Summary).
# ESTRUCTURA DE SALIDA
## SECCI√ìN 1: Encabezado
* T√≠tulo Corto, Subt√≠tulo y Etiquetas.
## SECCI√ìN 2: El Sem√°foro de Cambios
* ROJO (STOP): Pr√°cticas a abandonar.
* AMARILLO (PRECAUCI√ìN): √Åreas de incertidumbre.
* VERDE (GO): Intervenciones recomendadas.
## SECCI√ìN 3: Algoritmo de Flujo
* Diagrama de flujo l√≥gico paso a paso.
## SECCI√ìN 4: "The Big Numbers"
* Cifras clave (dosis, tiempos, umbrales) para poner en grande.
## SECCI√ìN 5: Resumen Ejecutivo
* 3 Mensajes para llevar a casa.
* Nivel de Evidencia global.
"""

# --- FUNCIONES ---
def get_pdf_text(pdf_file):
    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() or ""
        return text
    except Exception as e:
        return None

# --- INTERFAZ ---
with st.sidebar:
    st.image("https://cdn-icons-png.flaticon.com/512/3063/3063176.png", width=80)
    # NOMBRE ACTUALIZADO EN BARRA LATERAL
    st.title("Medical Critical Care Hub")
    st.markdown("**Dr. Herbert Baquerizo Vargas**")
    st.caption("Althaia, Xarxa Assistencial Universit√†ria de Manresa")
    st.divider()
    
    if "GOOGLE_API_KEY" in st.secrets:
        st.success("‚úÖ Licencia Activada")
    else:
        st.error("‚ö†Ô∏è Falta API Key en Secrets")
        
    st.divider()
    uploaded_file = st.file_uploader("Subir Gu√≠a (PDF)", type=['pdf'])

# --- L√ìGICA PRINCIPAL ---
try:
    api_key = st.secrets["GOOGLE_API_KEY"]
except:
    api_key = None

if uploaded_file and api_key:
    genai.configure(api_key=api_key)
    
    # --- AUTO-SELECCI√ìN DE MODELO (Mantenemos la l√≥gica que funcion√≥) ---
    if "target_model" not in st.session_state:
        try:
            available = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
            # Prioridad: Flash > Pro > Cualquiera
            if any('flash' in m for m in available):
                st.session_state.target_model = next(m for m in available if 'flash' in m)
            elif any('pro' in m for m in available):
                st.session_state.target_model = next(m for m in available if 'pro' in m)
            elif available:
                st.session_state.target_model = available[0]
            else:
                st.error("No se encontraron modelos disponibles.")
        except:
            st.session_state.target_model = 'models/gemini-pro'

    # Configuramos el modelo
    model = genai.GenerativeModel(st.session_state.target_model)
    
    # Procesar PDF
    if "pdf_text" not in st.session_state or st.session_state.get("file_name") != uploaded_file.name:
        with st.spinner(f"Procesando documento con {st.session_state.target_model}..."):
            text = get_pdf_text(uploaded_file)
            st.session_state.pdf_text = text
            st.session_state.file_name = uploaded_file.name
            st.session_state.chat_history = []
    
    st.success(f"Gu√≠a cargada: {uploaded_file.name}")
    
    # --- PESTA√ëAS ---
    tab1, tab2, tab3 = st.tabs(["üìã An√°lisis Cl√≠nico", "üé® Infograf√≠a", "üí¨ Chat con la Gu√≠a"])

    # TAB 1: AN√ÅLISIS
    with tab1:
        st.header("An√°lisis Delta & Bedside")
        if st.button("Generar Informe Intensivista", key="btn_analisis"):
            with st.spinner("Analizando evidencia..."):
                try:
                    full_prompt = PROMPT_ANALISIS + "\n\nDOCUMENTO:\n" + st.session_state.pdf_text
                    response = model.generate_content(full_prompt)
                    st.markdown(response.text)
                except Exception as e:
                    st.error(f"Error: {e}")

    # TAB 2: INFOGRAF√çA
    with tab2:
        st.header("Dise√±o Visual")
        if st.button("Generar Estructura Visual", key="btn_info"):
            with st.spinner("Estructurando datos..."):
                try:
                    full_prompt = PROMPT_INFOGRAFIA + "\n\nDOCUMENTO:\n" + st.session_state.pdf_text
                    response = model.generate_content(full_prompt)
                    st.markdown(response.text)
                except Exception as e:
                    st.error(f"Error: {e}")

    # TAB 3: CHATBOT
    with tab3:
        st.header("Interrogar al PDF")
        for msg in st.session_state.chat_history:
            st.chat_message(msg["role"]).write(msg["content"])
        
        if prompt := st.chat_input("Ej: ¬øDosis de carga? ¬øContraindicaciones?"):
            st.chat_message("user").write(prompt)
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            
            try:
                chat_prompt = f"Act√∫a como experto m√©dico. Contexto de la gu√≠a:\n{st.session_state.pdf_text}\n\nPregunta: {prompt}\nRespuesta:"
                resp = model.generate_content(chat_prompt)
                st.chat_message("assistant").write(resp.text)
                st.session_state.chat_history.append({"role": "assistant", "content": resp.text})
            except Exception as e:
                st.error(f"Error respondiendo: {e}")

elif not api_key:
    st.warning("‚ö†Ô∏è Configura la API Key en los Secrets.")
